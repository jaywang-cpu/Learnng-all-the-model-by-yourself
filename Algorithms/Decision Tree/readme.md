# Decision Tree

## Definition 
Decision Tree is a supervised learning algorithm that creates a tree-like model of decisions to predict target values through recursiveï¼ˆé€’è¿›çš„ï¼‰ binary splitting. 
Just like a doctor's diagnosis: step by step narrowing down the possibilities based on symptoms to reach a final diagnosis! ğŸŒ³
![](https://github.com/jaywang-cpu/Learnng-all-the-model-by-yourself/blob/main/figure/Algorithms/Structure%20of%20decision%20tree.png)
* leaf node: four pie charts
* parent node: Expression correlation>0.9
* children nodeï¼šthose in the middle
----------------------------
## How to evalute it 
### 1. Entropy
#### *Defintion*
we use entropy to quantify the similarity or difference in the node, stands for the chaosity within the node, more close to 0 more close to purity, 1 means half-half possibility which have the most impurity.

#### *Calculation*
The easiest way to calculate it :https://www.youtube.com/watch?v=YtebGVx-Fxw (very easy to understand)
![](https://github.com/jaywang-cpu/Learnng-all-the-model-by-yourself/blob/main/figure/Algorithms/Entropy.jpg)

### 2. Gini Index

#### *Definition*
We use Gini index to measure the impurity or inequality within a node. It represents the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the node. Gini = 0 indicates perfect purity (all samples belong to one class), while higher values indicate more impurity.

#### *Calculation*
The Gini index is calculated as:
**Gini = 1 - Î£(pi)Â²**

Where pi is the probability of class i in the node.

For a binary classification:
- If all samples are the same class: Gini = 0 (pure)
- If samples are evenly split (50-50): Gini = 0.5 (maximum impurity)

#### *Example*
Node with 100 samples: 60 Class A, 40 Class B
- P(A) = 60/100 = 0.6
- P(B) = 40/100 = 0.4
- Gini = 1 - (0.6Â² + 0.4Â²) = 1 - (0.36 + 0.16) = 0.48

### 3.Key difference from Entropy
Gini index is computationally faster (no logarithms) and ranges from 0 to 0.5 for binary classification, while entropy ranges from 0 to 1.

-------------------------------------
## Important concept

| æ¦‚å¿µ | ç®€å•å®šä¹‰ | å…¬å¼/æ€ä¹ˆç”¨ | ä¸ºä»€ä¹ˆé‡è¦ |
|------|----------|-------------|------------|
| **Information Gain** | è¡¡é‡åˆ†å‰²åä¿¡æ¯çš„æ”¹å–„ç¨‹åº¦ | `the entropy of parents substract children's` | é€‰æ‹©æœ€å¥½çš„åˆ†å‰²ç‰¹å¾ ï¼Œè¶Šå¤§è¶Šå¥½|
| **Information Gain Ratio** | ä¿®æ­£ç‰ˆçš„ä¿¡æ¯å¢ç›Š | `Gain Ratio = Information Gain / Intrinsic Information` | è®©é€‰æ‹©æ›´å…¬å¹³ |
| **Pre-pruning (é¢„å‰ªæ)** | æå‰åœæ­¢é•¿æ ‘ | `max_depth=5`, `min_samples_split=10` | é˜²æ­¢æ ‘é•¿å¾—å¤ªå¤æ‚ |
| **Post-pruning (åå‰ªæ)** | ç æ‰æ²¡ç”¨çš„æ ‘æ | å…ˆé•¿å®Œæ•´æ ‘ï¼Œå†åˆ é™¤å¤šä½™éƒ¨åˆ† | è®©æ ‘æ›´ç®€æ´æœ‰æ•ˆ |
| **è¿ç»­ç‰¹å¾åˆ†å‰²Continuous Feature Splitting** | å¤„ç†æ•°å­—æ•°æ® | æ‰¾ä¸ªæ•°å­—å½“åˆ†ç•Œçº¿ï¼š`age â‰¤ 30` | èƒ½å¤„ç†å¹´é¾„ã€æ”¶å…¥ç­‰æ•°æ® |
| **ç±»åˆ«ç‰¹å¾åˆ†å‰²Categorical Feature Splitting** | å¤„ç†æ–‡å­—æ•°æ® | æŒ‰ç±»åˆ«åˆ†ç»„ï¼š`color = red/blue/green` | èƒ½å¤„ç†é¢œè‰²ã€æ€§åˆ«ç­‰æ•°æ® |
| **è¿‡æ‹Ÿåˆé—®é¢˜Overfitting Problem** | æ ‘è®°ä½äº†è®­ç»ƒæ•°æ®çš„ç»†èŠ‚ | åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°å·® | éœ€è¦æ§åˆ¶æ ‘çš„å¤æ‚åº¦ |
| **ç‰¹å¾åå‘Feature Bias** | åçˆ±"é€‰é¡¹å¤š"çš„ç‰¹å¾ | æ¯”å¦‚é€‰æ‹©"åŸå¸‚"è€Œä¸æ˜¯"æ€§åˆ«" | å¯èƒ½é€‰é”™é‡è¦ç‰¹å¾ |
| **ç¼ºå¤±å€¼å¤„ç†Missing Values Handling** | æœ‰äº›æ•°æ®ç©ºç™½æ€ä¹ˆåŠ | ç”¨å…¶ä»–æ–¹æ³•å¡«è¡¥æˆ–å•ç‹¬å¤„ç† | è®©ç®—æ³•æ›´å®ç”¨ |



