# Overview
**1.definition** <br>
**2.principle**<br>
3.


# Definition

## what is KNN
k-Nearest Neighbors 是一种基于实例的监督学习算法，通过寻找K个最相思的邻居来对新数据点进行分类或者回归预测（核心细想：物以类聚），最核心的四个步骤便是，找距离，选择邻居个数，选出邻居，决定规则

## 算法特点
- lazy learning: 无需训练，直接存储所有训练数据
- Non-parametric: 不对数据分布作任何假设
- Instance-learning：基于具体实例进行预测

## 四个核心要素

### 距离（Distance)
- 欧几里得距离：直线距离，最常用 90%：  sqrt((x1-y1)² + (x2-y2)² + ... + (xn-yn)²)
- 曼哈顿距离：网格距离，适合高位数据：  |x1-y1| + |x2-y2| + ... + |xn-yn|
- 闵可夫斯基距离：通用万能距离公式：    (|x1-y1|^p + |x2-y2|^p + ... + |xn-yn|^p)^(1/p)  

### 邻居个数（K值）
- 决定多少个最近邻居进行决策
- 通常选择奇数，避免后面投票是平票
- 常用值：3，5，7，9
- 大小容易过拟合（只听一个），太大容易欠拟合（谁的都听）

### 确定邻居
- 计算新点到训练点的数据
- 按距离从小到大排序
- 选前K个做为邻居

### 最后判决（Final Decision)
- 分类任务：多数投票（Majority Voting）：5个里面3个垃圾邮件-预测为垃圾邮件
- 回归任务：取平均值（Average）：3个邻居，1-300w，2-320w，3-280w，那么这个新数据取平均值就行
  
